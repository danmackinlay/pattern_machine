arpeggiate by numbers
========================


##  MDS version

Learning a harmony space

assumptions: 

* all notes are truncated saw waves with 16 harmonics
* all harmonics wrapped to the octave

### MDS Todo

* Not all distances are equally important; the ones beween two dissonant chords are not significant.
  Can i distort space, or change weightings, to deal with this?
* If I threw out all chords with more than 6 notes I would also speed up search times. Just sayin'.
* weight by actual chord occurence (when do 11 notes play at once? even 6 is pushing it)
* restrict cursor to convex hull of notes, or, e.g. ball?
* exploit cyclic permutation in distance calculations - and analysis
   e.g. hmmm we could position pitch classes rotationally around an axis
   note further that there might be yet more symmetry relationship dependednt
   on the class - but sets of 12 is an obvious one.
   * Aside: interesting algebra. which one is it?
   * Looks a little like the Boolean algebra (hazy)
   * The quotion ring wrt the operation ideas generated by transposition (i.e. circular/cyclic permutation)?
   * Not quite a symmetric group, hey? since always a subgroup. Multiset permutation group?
   * hum, sure I can construct it as an algebra over the power set of {0,..., 11} https://en.wikipedia.org/wiki/Algebra_of_sets *vagues out*
* toroidal dist maps for that damn kernel
* simple transition graph might work, if it was made regular in some way
* we could even place chords on a grid in a way that provides minimal dissonance between them; esp since we may repeat chords if necessary. In fact, we could even construct such a path by weaving chords together. Hard to navigate, without some orderings
* why not transform these distances in some way, some kinda monotonic transform that makes the connectivity graph sparse.
* a physics-based model might do this reasonably well - springs with constants monotonic in product
* colorize based on number of notes
* Actually integrate kernels together
* use gram matrix as a markov transition probability weight in some kind of deranged markov model (you'd want some weighting or restriction)
* ditch pickle for optimized tables https://pytables.github.io/usersguide/optimization.html
* remove chord 0 (silence), since it only causes trouble.
* switch to JSON for interchange medium
* RBF spectral embedding with a variable gamma could produce a nice colour scheme, hm?
* visualise, somehow, e.g.
  * http://www.ibm.com/developerworks/library/wa-webgl3/
  * http://scenejs.org/
  * http://threejs.org/
  
## Dissonance curves

See Benson's "Music: A mathematical offering" for some nice overviews; but the reference implementation seems to be dissmeasure and dissmeasure2 from Lach Lau's [DissonanceLib](https://github.com/supercollider-quarks/DissonanceLib/blob/master/classes/Dissonance.sc):

## Other techniques

I was kinda attracted to doing this as a cellular automata, but that was a horrible mess; too much structure outside of my learning.

Can I recover it?

It might be fun to do so by somply looking at outcome rows and eliminating from consideration of them all prior rows which did not help.


### Graphical models

#### Directed

easy to generate from; hard to see that its natural though.

Although maybe if past notes only ever caused future we would be fine.
Practically, though, a pianist will anticipate, so maybe we should just go for an undirected random field.

#### (Semi-?)Markov random field

We regress the conditional occurence of each note against the algrebra generated by past notes.

So that would be the regression of 2^12 interaction terms
(possibly constrained by rotational symmetry, possibly assumed e.g. linear and pairwise symmetric and hence 12*11 terms)
against 12*window past values.

Gibbs samplers of distribution of notes.

What is our energy functional? Dissonance? Dissonance per note?

Spectral autocorrelation inference?
In log-probability?
in log-RATE?



### Linear-style regression

Could do various things here;

* Generalized additive models.
* nonparametric propensity scores
* What now seems most natural: linear self-excited (discrete hawkes) processes, where we regress a *rate* kernel and possibly interaction terms (which must be positive but at least we can do logarithmic regression).
  
* but what I tried was logistic regression of self against past

This might be quicker with [SGD](http://scikit-learn.org/stable/modules/sgd.html#sgd):

    mod = SGDClassifier(loss="log", penalty="l1", shuffle=True)
  

### Reinforcement/ MDP models

Agents could learn to "play" against one another to form consonances?
Not clear what the loss and rewards functiosn should be here to keep it dynamic.

### Branching process

If we could work out how to do a periodic kernel this could be sweet.
But it is non-sparse regression in 
(tones ⨉ wavelengths ⨉ 2 (for phase)) ^ interactions.
SGD?
Expectation maximisation?
Could do a kernel-recurrence relation a la Wheatley.
* Or regress against something time-bound, perhaps...

  * decaying sinusoidal impulses? but with what period? likely several harmonics of note length.
  * interaction terms?
  * What decay? No idea. Even several superposed decays could be natural. Would have to fit term decay, which would not be linear.
  * this might possibly work via some kind of iterative method such as expectation maximisation, or just normal quasi-newton optimisation even; it would be polynomial of order no great than degree of interactions tested, which would be exactly automatically differentiable
  * How would we handle phase?



## other TODO

* handle multiplicity of note events using a point-process rate model.
* [hint hdf chunk size](http://pytables.github.io/usersguide/optimization.html#informing-pytables-about-expected-number-of-rows-in-tables-or-arrays)
* [trim the data set](http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/#how_large_the_training_set_should_be?)
* [A list of alternate datasets](http://notes.livingthing.org/musical_corpora.html).
