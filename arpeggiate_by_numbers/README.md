arpeggiate by numbers
========================


##  MDS version

Learning a harmony space

assumptions: 

* all notes are truncated saw waves with 16 harmonics
* all harmonics wrapped to the octave

### MDS Todo

* Dissonance kernels?
* Not all distances are equally important; the ones beween two dissonant chords are not significant.
  Can i distort space, or change weightings, to deal with this?
* If I threw out all chords with more than 6 notes I would also speed up search times. Just sayin'.
* weight by actual chord occurence (when do 11 notes play at once? even 6 is pushing it)
* restrict cursor to convex hull of notes, or, e.g. ball?
* exploit cyclic permutation in distance calculations - and analysis
   e.g. hmmm we could position pitch classes rotationally around an axis
   note further that there might be yet more symmetry relationship dependednt
   on the class - but sets of 12 is an obvious one.
   * Aside: interesting algebra. which one is it?
   * Looks a little like the Boolean algebra (hazy)
   * The quotion ring wrt the operation ideas generated by transposition (i.e. circular/cyclic permutation)?
   * Not quite a symmetric group, hey? since always a subgroup. Multiset permutation group?
   * hum, sure I can construct it as an algebra over the power set of {0,..., 11} https://en.wikipedia.org/wiki/Algebra_of_sets *vagues out*
* toroidal dist maps for that damn kernel
* simple transition graph might work, if it was made regular in some way
* we could even place chords on a grid in a way that provides minimal dissonance between them; esp since we may repeat chords if necessary. In fact, we could even construct such a path by weaving chords together. Hard to navigate, without some orderings
* why not transform these distances in some way, some kinda monotonic transform that makes the connectivity graph sparse.
* a physics-based model might do this reasonably well - springs with constants monotonic in product
* colorize based on number of notes
* Actually integrate kernels together
* use gram matrix as a markov transition probability weight in some kind of deranged markov model (you'd want some weighting or restriction)
* ditch pickle for optimized tables https://pytables.github.io/usersguide/optimization.html
* remove chord 0 (silence), since it only causes trouble.
* switch to JSON for interchange medium
* RBF spectral embedding with a variable gamma could produce a nice colour scheme, hm?
* visualise, somehow, e.g.
  * http://www.ibm.com/developerworks/library/wa-webgl3/
  * http://scenejs.org/
  * http://threejs.org/

## Other techniques

I was kinda attracted to doing this as a cellular automata, but that was a horrible mess; too much structure outside of my learning.

Can I recover it?

It might be fun to do so by somply looking at outcome rows and eliminating from consideration of them all prior rows which did not help.


### PGM

Could go to a discrete PGM model, such as

* [catnet](http://cran.r-project.org/web/packages/catnet/vignettes/catnet.pdf)
* [gRaphHD](http://www.jstatsoft.org/v37/i01/)
* [bnlearn](http://www.bnlearn.com/)
* [R overview](https://r-forge.r-project.org/R/?group_id=1487)

but let's stay simple and start with a generalized linear model of some
description.

In any case I don't think that works; there is definitely hidden state,
possibly only probabilistic state. See HMM.

* more generous compound feature search which allows features to appear which
  are *ONLY* interaction terms, despite both parents not being significant
  
  * Well, the principled way of finding the maximally broad principled way of doing this is precicely the PC algorithm.
  * (To think: should i then make a graph of all interaction terms?)
  * I use the PC-algorithm to find parents of the note sounding thing, then either use that conditional distribution table, or regress against the parent set.
  * this could be sped up (if supported) by enforcing causal arrow directions between timesteps

* would this mean i should use the model as is? implement graphical model outputs in SC?


### Linear-style regression

Could do various things here;

* Generalized additive models.
* nonparametric propensity scores
* What now seems most natural: linear self-excited (discrete hawkes) processes, where we regress a *rate* kernel and possibly interaction terms (which must be positive but at least we can do logarithmic regression).
  
* but what I tried was logistic regression of self against past

This might be quicker with [SGD](http://scikit-learn.org/stable/modules/sgd.html#sgd):

    mod = SGDClassifier(loss="log", penalty="l1", shuffle=True)
  

### *sigh* Hidden Markov Models

Are actually not bad.

* Python/C http://ghmm.org/
* http://pandamatak.com/people/anand/771/html/node26.html

### Reinforcement/ MDP models

Agents could learn to "play" against one another to form consonances?
Not clear what the loss and rewards functiosn should be here to keep it dynamic.

### Branching process

If we could work out how to do a periodic kernel this could be sweet.
But it is non-sparse regression in 
(tones ⨉ wavelengths ⨉ 2 (for phase)) ^ interactions.
SGD?
Expectation maximisation?
Could do a kernel-recurrence relation a la Wheatley.

* Reynaud-Bouret, P., & Schbath, S. (2010). Adaptive estimation for Hawkes processes; application to genome analysis. *The Annals of Statistics*, 38(5), 2781–2822. `DOI <http://dx.doi.org/10.1214/10-AOS806>`__. `Online <http://projecteuclid.org/euclid.aos/1279638540>`__. 
* Or regress against something time-bound, perhaps...
  * decaying sinusoidal impulses? but with what period? likely several harmonics of note length.
  * What decay? No idea. Even several superposed decays could be natural. Would have to fit term decay, which would not be linear
  * this might possibly work via some kind of iterative method such as expectation maximisation, or just normal newton-raphson optimisation even; it would be polynomial of order no great than degree of interactions tested, which would be exactly automatically differentiable
  * How would we handle phase? probably by regressing against componenets of an imaginary wave separatedly.

### Semi-Markov random field

We regress the conditional occurence of each note against the algrebra generated by past notes.

So that would be the regression of 2^12 interaction terms
(possibly constrained by rotational symmetry, possibly assumed e.g. linear and pairwise symmetric and hence 12*11 terms)
against 12*window past values.

This could also be weird, if our 


## other TODO

* handle multiplicity of note events?
  * naive model: recentness versus relative pitch, linear in each. This would be sorta easy to implement. Should we also regress on correct value for recentness then?

* [hint hdf chunk size](http://pytables.github.io/usersguide/optimization.html#informing-pytables-about-expected-number-of-rows-in-tables-or-arrays)
* [trim data the set](http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/#how_large_the_training_set_should_be?)
* Switch to pure python using [liblinear](http://www.csie.ntu.edu.tw/~cjlin/liblinear/)
  
  NB Maybe not. Very slow in liblineaR atm - no nice optimisations for logistic regression or binary factors as in glmnet/R

* predict inter-event times - would be a natural multiple classification task

* [A list of alternate datasets](http://notes.livingthing.org/musical_corpora.html).
